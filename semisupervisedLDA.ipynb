{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import guidedlda\n",
    "import numpy as np\n",
    "data=pd.read_csv('post_done.csv',encoding='ISO-8859-1')\n",
    "data_text=data[['post_content']]\n",
    "data_text['index']=data_text.index\n",
    "docs=data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_content</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>=IF(RAND()) function</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"const\" in =Trend (known_y, known_x, new_x, co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-2017 Term 2 Final Exam Qn 2 Part D</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5th degree polynomial</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A way to show multiple solutions for Solver?</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        post_content  index\n",
       "0                               =IF(RAND()) function      0\n",
       "1  \"const\" in =Trend (known_y, known_x, new_x, co...      1\n",
       "2            2016-2017 Term 2 Final Exam Qn 2 Part D      2\n",
       "3                              5th degree polynomial      3\n",
       "4       A way to show multiple solutions for Solver?      4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['formula','change','column','drag','cell','thank','value','would','i','dragging','stupid','a','b','c','dragged','however','made','hope','isnt','prof','let','according','lesson','merely','think','hi','use','used','using','get','may','mistake','small','believe','instead','done','uploaded','part','everybody','learnt','refers','means','typo','hey','oh','sorry','misconception','probably','regarding','nvm','hmm','thanks','help','ambiguous','wrong','type','mentioned','slide','remain','haha','gave','earlier','notice','aside','slides','yeah','others','today','wrt','yes','refer','also','doubt','const','constant','guys','anyone','like','want','answer','solution','wondering','explanations','suggestion','answers','solutions','see','could','check','different','ans','helps','excel','select','scenario','takes','simple','explanation','sample','eg','option','actually','simplified','fill','confusing','easier','click','well','n','u','try','tried','confirm','run','difficult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''def stemming(text):\n",
    "    return stemmer.stem(text)\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in stop_words and len(token) > 3:\n",
    "            result.append(stemming(token))\n",
    "    return result'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "''''from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "import string\n",
    "def preprocess(transcript):\n",
    "    nopunc = [char for char in transcript if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    stop_removed=[word for word in nopunc.split() if word.lower() not in stop_words]\n",
    "    return [stemmer.stem(w) for w in stop_removed]''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'doc_sample = docs[docs['index'] == 10].values[0][0]\\nprint('original document: ')\\nwords = []\\nfor word in doc_sample.split(' '):\\n    words.append(word)\\nprint(words)\\nprint('\\n\\n tokenized and stemmed document: ')\\nprint(preprocess(doc_sample))\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''''doc_sample = docs[docs['index'] == 10].values[0][0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and stemmed document: ')\n",
    "print(preprocess(doc_sample))'''''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list1=['COUNT','COUNTIF','COUNTIFS','CONCATENATE','SUM','SUMIF','SUMIFS','AutoFilter','AVERAGE','AVERAGEIF','AVERAGEIFS','ROUND','ROUNDUP','ROUNDDOWN','INT','MAX','MIN','STDEV','SLOPE','INTERCEPT','LOOKUP','VLOOKUP','HLOOKUP','MATCH','INDEX','match type','Approximate Match','Exact Match','Time Value','PMT','PV','FV','NPER','IRR','RATE','NPV','RAND','RANDBETWEEN','SMALL','LARGE','PERCENTILE','TODAY','NOW','YEAR','MONTH','DAY','DATE','TIME','HOUR','MINUTE','SECOND','LN','EXP','Probability Distributions','Discrete Distribution','Continuous Distribution','NORMDIST','NORMSDIST','NORMINV','NORMSINV','CRITBINOM','BINOMDIST','BINOM.INV','POISSON','EXPONDIST','Binomial','Exponential','Normal','Uniform','PDF','PMF','CDF','CRF table','FREQUENCY','SQRT','FILL','AUTO-FILL','Advanced Filter','Filter','Alex Processing','Achilles and Tortoise','F1 Night City Race','Sensitivity analysis','Trade-off analysis','R-squared','Village Coffee','Visual Basic Editor','Black-Scholes','Charity Donation','Text to Columns','From Web','Query file','Echo Office Supplies','Goal Seek','Linear Programming','Solver','Objective function','Constraints','Decision variables','Solving Method','breakeven','maximum solution','minimum solution','optimal solution','binary','integer','Monte Hall','Data Simulation','XDB Bank','Queue Simulation','Timer-Clicker','ABC Services','Date Formats','Pivot Table','Pivot Chart','Macro','VBA','Referencing','Relative Referencing','Mixed Referencing','Absolute Referencing','Trace','Retail Gasoline','Multiplication Table','CCH Kindergarten','Grand Grocery','Frequency Distribution','price-demand relationship','Scatter','Graph','Series','Trend','Forecast','IF','IFS','Nested IF','TEXT','IFERROR','AND','OR','Criteria','Sheet','Conditional Formatting','SUMPRODUCT','Data Validation','Array Formula','Circular Reference','What-if Analysis','Data Table','Row Input Cell','Column Input Cell','OFFSET','ISTEXT','ISODD','ISEVEN','ISNA','ISERROR','ISBLANK','Format','ABS','TRENDLINE','Polynomial','Influence Diagram','Blackbox Model','Decision Model','Revenue','Cost','Profit','Profit Margin','Contribution Margin','Group Project','Documentation','Assignment 0','Assignment 1','Assignment 2','Revision','Sample Exam','Quiz 1','Quiz 2','Fast key','New Function','Time Formats','Excel Options','External links','Formulas']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list2=[w.lower() for w in list1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(a):\n",
    "    a0=nltk.word_tokenize(a)\n",
    "    a1=[w.lower() for w in a0]\n",
    "    a2=[i for i in a1 if re.search('^[a-z]+$',i)]\n",
    "    a3= [word for word in a2 if word not in stop_words]\n",
    "    a4=[stemmer.stem(w) for w in a3]\n",
    "    a5=[word for word in a4 if word not in stop_words]\n",
    "    #a6=[word for word in a5 if word in list2]\n",
    "    return a5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[df['post_content'].isin([])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = docs['post_content'].apply(preprocess)\n",
    "#processed_doc=processed_docs.map(stop_words_removal())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_docs.to_csv('docs_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(208 unique tokens: ['function', 'rand', 'exam', 'final', 'qn']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2=pd.read_csv('docs_processed.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=data_2['post'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_topic = { 'alex':0, 'process':0, 'roundup':0, 'math':0, 'achil':0,'referenc':0,'tortiois':0,\n",
    "                   'night':1, 'race':1,'polynomi':1,'profit':1,'cost':1,'margin':1,'trend':1,'blackbox':1,'contribut':1,'breakeven':1,'break':1,'even':1,'optim':1,'solver':1,\n",
    "                   'pmt':2,'fv':2,'nper':2,'npv':2,'pv':2,\n",
    "                   'lookup':3,'vlookup':3,'hlookup':3,'echo':3,'office':3,'offic':3,'suppli':3,'index':3,'match':3,'linear':3,'format':3,'echoofficesuppli':3,\n",
    "                   'carlo':4,'hall':4,'distribut':4,'exponenti':4,'frequenc':4,'simul':4,'mont':4,'poisson':4,'normdist':4,\n",
    "                   'time':5,'today':5,'date':5,'xdbbank':5,'xdb':5,'date':5,\n",
    "                   'vba':6 ,'macro':6\n",
    "                  \n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 1025\n",
      "INFO:guidedlda:vocab_size: 2227\n",
      "INFO:guidedlda:n_words: 24921\n",
      "INFO:guidedlda:n_topics: 10\n",
      "INFO:guidedlda:n_iter: 100\n",
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\guidedlda\\utils.py:55: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if sparse and not np.issubdtype(doc_word.dtype, int):\n",
      "INFO:guidedlda:<0> log likelihood: -285378\n",
      "INFO:guidedlda:<20> log likelihood: -182898\n",
      "INFO:guidedlda:<40> log likelihood: -178864\n",
      "INFO:guidedlda:<60> log likelihood: -177402\n",
      "INFO:guidedlda:<80> log likelihood: -175871\n",
      "INFO:guidedlda:<99> log likelihood: -175647\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<guidedlda.guidedlda.GuidedLDA at 0x28947623438>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = guidedlda.GuidedLDA(n_topics=10, n_iter=100, random_state=7, refresh=20)\n",
    "model.fit(X, seed_topics=seed_topic, seed_confidence=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: day year date total month cost unit margin number februari\n",
      "Topic 1: number round function neg code rate invest exampl irr minut\n",
      "Topic 2: question distribut function model probabl mean randbetween normal rand number\n",
      "Topic 3: cost question profit time increas demand order case model higher\n",
      "Topic 4: number rang door sum count text sheet enter charact function\n",
      "Topic 5: time number rand end averag simul data gener servic order\n",
      "Topic 6: solver constraint set valu variabl problem method integ optim find\n",
      "Topic 7: row array match tabl valu vlookup return period index lookup\n",
      "Topic 8: function number machin rand give error valu gener data result\n",
      "Topic 9: function condit fals true return data set lookup match find\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 10\n",
    "topic_word = model.topic_word_\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary.filter_extremes(no_above=0.8, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (4, 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_doc_10 = bow_corpus[10]\n",
    "for i in range(len(bow_doc_10)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_10[i][0], \n",
    "                                               dictionary[bow_doc_10[i][0]], \n",
    "bow_doc_10[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.27827955249080416),\n",
      " (1, 0.2815250629833237),\n",
      " (2, 0.37073037565740413),\n",
      " (3, 0.37468762098366154),\n",
      " (4, 0.2752218390274539),\n",
      " (5, 0.35101823691498624),\n",
      " (6, 0.24544546637547687),\n",
      " (7, 0.426228303180853),\n",
      " (8, 0.3529835663356211)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.400*\"cost\" + 0.105*\"index\" + 0.043*\"countif\" + 0.043*\"rate\" + 0.040*\"nper\" + 0.039*\"randbetween\" + 0.032*\"time\" + 0.029*\"format\" + 0.026*\"day\" + 0.025*\"match\"\n",
      "Topic: 1 \n",
      "Words: 0.221*\"match\" + 0.138*\"day\" + 0.118*\"lookup\" + 0.108*\"month\" + 0.081*\"index\" + 0.078*\"date\" + 0.047*\"year\" + 0.040*\"time\" + 0.039*\"vlookup\" + 0.028*\"rand\"\n",
      "Topic: 2 \n",
      "Words: 0.178*\"solver\" + 0.145*\"date\" + 0.126*\"pmt\" + 0.104*\"time\" + 0.095*\"pv\" + 0.056*\"month\" + 0.047*\"int\" + 0.035*\"second\" + 0.027*\"max\" + 0.020*\"npv\"\n",
      "Topic: 3 \n",
      "Words: 0.176*\"year\" + 0.133*\"time\" + 0.099*\"fv\" + 0.069*\"vba\" + 0.069*\"text\" + 0.061*\"istext\" + 0.050*\"macro\" + 0.037*\"graph\" + 0.032*\"forecast\" + 0.032*\"scatter\"\n",
      "Topic: 4 \n",
      "Words: 0.157*\"time\" + 0.144*\"format\" + 0.127*\"rate\" + 0.068*\"sum\" + 0.068*\"normal\" + 0.060*\"text\" + 0.053*\"normdist\" + 0.047*\"normsdist\" + 0.032*\"trend\" + 0.030*\"day\"\n",
      "Topic: 5 \n",
      "Words: 0.176*\"year\" + 0.175*\"profit\" + 0.173*\"solver\" + 0.140*\"cost\" + 0.057*\"day\" + 0.038*\"breakeven\" + 0.034*\"graph\" + 0.029*\"lookup\" + 0.027*\"rate\" + 0.021*\"month\"\n",
      "Topic: 6 \n",
      "Words: 0.227*\"count\" + 0.163*\"second\" + 0.090*\"match\" + 0.087*\"text\" + 0.074*\"time\" + 0.069*\"graph\" + 0.065*\"format\" + 0.046*\"lookup\" + 0.041*\"max\" + 0.019*\"round\"\n",
      "Topic: 7 \n",
      "Words: 0.403*\"time\" + 0.108*\"randbetween\" + 0.083*\"rand\" + 0.067*\"min\" + 0.055*\"hour\" + 0.037*\"second\" + 0.037*\"irr\" + 0.031*\"npv\" + 0.023*\"rate\" + 0.020*\"day\"\n",
      "Topic: 8 \n",
      "Words: 0.152*\"sheet\" + 0.145*\"vlookup\" + 0.126*\"time\" + 0.118*\"lookup\" + 0.066*\"normal\" + 0.059*\"sum\" + 0.056*\"month\" + 0.046*\"date\" + 0.037*\"format\" + 0.034*\"hlookup\"\n",
      "Topic: 9 \n",
      "Words: 0.280*\"rand\" + 0.184*\"round\" + 0.119*\"trend\" + 0.100*\"max\" + 0.058*\"roundup\" + 0.040*\"min\" + 0.024*\"ln\" + 0.022*\"uniform\" + 0.021*\"format\" + 0.020*\"rounddown\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
